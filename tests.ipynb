{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7038732f-b120-4d16-afa9-7ebcdb7e8a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a586e84-021d-4f07-a897-45349f2e9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "43131b97-0ac8-4538-8c5c-194cdca5690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb579deb-35de-41ae-ac54-c62fd3091a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = datasets.load_from_disk(\"./train.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af7db622-28e8-4ae0-974c-cda43ed3f918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 989529\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c8b3ef2a-d0bf-490b-b31d-3081e422e2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hmm. ###>Hmm. _',\n",
       " 'Mundurlah, bajingan. ###>Back the fuck up!',\n",
       " 'Rebecca... Pikirkanlah baik-baik. ###>Rebecca have a good think about it.',\n",
       " \"Kau salah. ###>You're wrong.\",\n",
       " 'Brook, apa kau ada perkataan untuk dirimu? ###>Brook, what do you have to say for yourself?',\n",
       " 'Katakan. ###>Tell me.',\n",
       " \"Kita mulai dengan cambuk pendek. ###>We'll start with the riding crop.\",\n",
       " 'Oh, aku tahu itu. ###>Oh, I know it.',\n",
       " \"Tidak bisa . ###>I... can't.\",\n",
       " 'Dan apa yg dia selipkan di bawah meja membuktikan kekuatan bahwa itu benar. ###>And what he slid under the table convinced the powers that be it was true.']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[\"text\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edcc3d-d5e3-4d41-822e-68550e79ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_find = [\n",
    "    \"##Feel\",\n",
    "    \"@\",\n",
    "]\n",
    "for sentence in ds_train[\"text\"]:\n",
    "    for tk in tokens_to_find:\n",
    "        if tk in sentence:\n",
    "            print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8a8d32f1-0c6a-419d-b90a-ec0b5ab0cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\"id\": set(), \"eng\": set()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "975a700c-29c0-46ba-84b9-a68c285f42c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_pattern = re.compile(r'^[a-zA-Z0-9\\s.,?!;:\\'\\\"-]+$')\n",
    "def is_valid_sample(s):\n",
    "    return bool(regex_pattern.match(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a4f533ed-d13d-4f58-8b7d-8b748377aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = [sentence for sentence in ds_train[\"text\"] if is_valid_sample(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f1741f9e-09b8-498e-8375-b31a1c4a414a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9d079d02-f4be-45c0-abda-cd01b54315db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['Brook',\n",
       "  ',',\n",
       "  'apa',\n",
       "  'kau',\n",
       "  'ada',\n",
       "  'perkataan',\n",
       "  'untuk',\n",
       "  'dirimu',\n",
       "  '?'],\n",
       " 'eng': ['Brook',\n",
       "  ',',\n",
       "  'what',\n",
       "  'do',\n",
       "  'you',\n",
       "  'have',\n",
       "  'to',\n",
       "  'say',\n",
       "  'for',\n",
       "  'yourself',\n",
       "  '?']}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer \n",
    "\n",
    "def tokenize(s):\n",
    "    id, eng = [*s.split(\"###>\")]\n",
    "    #print(\"id:\", id)\n",
    "    #print(\"eng:\", eng)\n",
    "    \n",
    "    ret = {\"id\": list(), \"eng\": list()}\n",
    "    regex = r'(\\s|,|!|\\?|\\.|:|;|\\'|\\\"|“|”|‘|’|\\(|\\)|\\[|\\]|\\{|\\})'\n",
    "    \n",
    "    # id\n",
    "    tks = re.split(regex, id.strip()) # split with various symbols (excluding hyphens)\n",
    "    for tk in tks:\n",
    "        if tk.replace(' ', '') != '':\n",
    "            ret[\"id\"].append(tk)\n",
    "    # eng\n",
    "    tks = re.split(regex, eng.strip())\n",
    "    for tk in tks:\n",
    "        if tk.replace(' ', '') != '':\n",
    "            ret[\"eng\"].append(tk)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "test = \"Brook, apa kau ada perkataan untuk dirimu? ###>Brook, what do you have to say for yourself?\"\n",
    "tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a61093b9-8399-430c-a226-046e6fb39505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['Selamat', 'pagi', '!', 'Apa', 'kabar', '?', '#Tanya'], 'eng': ['Good', 'morning', '!', 'How', 'are', 'you', '?', '#Question']}\n",
      "\n",
      "{'id': ['Saya', 'suka', 'bermain', 'sepak', 'bola', ',', 'terutama', 'di', 'akhir', 'pekan', '.', '@Sport'], 'eng': ['I', 'love', 'playing', 'soccer', ',', 'especially', 'on', 'weekends', '.', '@Football']}\n",
      "\n",
      "{'id': ['Kita', 'harus', 'pergi', 'ke', 'pasar', 'sekarang', '!', '#Urgent'], 'eng': ['We', 'have', 'to', 'go', 'to', 'the', 'market', 'now', '!', '#Important']}\n",
      "\n",
      "{'id': ['Buku', 'ini', 'sangat', 'menarik', ',', 'dan', 'saya', 'tidak', 'bisa', 'berhenti', 'membacanya', '.', '%Menarik'], 'eng': ['This', 'book', 'is', 'very', 'interesting', ',', 'and', 'I', 'can', \"'\", 't', 'stop', 'reading', 'it', '.', '%Interesting']}\n",
      "\n",
      "{'id': ['Apa', 'pendapatmu', 'tentang', 'film', 'itu', '?', '*Pikirkan'], 'eng': ['What', 'do', 'you', 'think', 'about', 'that', 'movie', '?', '*Think', 'about', 'it']}\n",
      "\n",
      "{'id': ['Hati-hati', '!', 'Jalan', 'licin', ',', 'terutama', 'saat', 'hujan', '.', '#HatiHati'], 'eng': ['Be', 'careful', '!', 'The', 'road', 'is', 'slippery', ',', 'especially', 'when', 'it', 'rains', '.', '#Caution']}\n",
      "\n",
      "{'id': ['Dia', 'berkata', ',', \"'\", 'Saya', 'akan', 'datang', 'besok', '.', \"'\", '#Janji'], 'eng': ['He', 'said', ',', \"'\", 'I', 'will', 'come', 'tomorrow', '.', \"'\", '#Promise']}\n",
      "\n",
      "{'id': ['Saya', 'tidak', 'suka', 'makanan', 'pedas', ';', 'lebih', 'baik', 'yang', 'manis', '.', '#Pilih'], 'eng': ['I', 'don', \"'\", 't', 'like', 'spicy', 'food', ';', 'I', 'prefer', 'sweet', '.', '#Choice']}\n",
      "\n",
      "{'id': ['Mengapa', 'kamu', 'terlambat', '?', 'Apakah', 'ada', 'masalah', '?', '@Tanya'], 'eng': ['Why', 'are', 'you', 'late', '?', 'Is', 'there', 'a', 'problem', '?', '@Question']}\n",
      "\n",
      "{'id': ['Bisa', 'kita', 'bicara', 'nanti', '?', 'Saya', 'sibuk', 'saat', 'ini', '.', '@Sibuk'], 'eng': ['Can', 'we', 'talk', 'later', '?', 'I', \"'\", 'm', 'busy', 'right', 'now', '.', '@Busy']}\n",
      "\n",
      "{'id': ['Cinta', 'itu', 'aneh', ',', 'bukan', '?', 'Terkadang', ',', 'sulit', 'untuk', 'dipahami', '.', '*Cinta'], 'eng': ['Love', 'is', 'strange', ',', 'isn', \"'\", 't', 'it', '?', 'Sometimes', ',', 'it', \"'\", 's', 'hard', 'to', 'understand', '.', '*Love']}\n",
      "\n",
      "{'id': ['Dia', 'membeli', 'apel', ',', 'jeruk', ',', 'dan', 'pisang', '.', '%Belanja'], 'eng': ['She', 'bought', 'apples', ',', 'oranges', ',', 'and', 'bananas', '.', '%Shopping']}\n",
      "\n",
      "{'id': ['Wow', '!', 'Itu', 'luar', 'biasa', ',', 'bukan', '?', '#Wow'], 'eng': ['Wow', '!', 'That', \"'\", 's', 'amazing', ',', 'isn', \"'\", 't', 'it', '?', '#Amazing']}\n",
      "\n",
      "{'id': ['Kamu', 'tidak', 'bisa', 'pergi', 'begitu', 'saja', ';', 'kita', 'harus', 'membahas', 'ini', '.', '@Bahas'], 'eng': ['You', 'can', \"'\", 't', 'just', 'leave', ';', 'we', 'need', 'to', 'discuss', 'this', '.', '@Discuss']}\n",
      "\n",
      "{'id': ['Setiap', 'orang', 'berhak', 'atas', 'pendapatnya', ',', 'bukan', '?', '*Pendapat'], 'eng': ['Everyone', 'has', 'the', 'right', 'to', 'their', 'opinion', ',', 'right', '?', '*Opinion']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the tokenizer\n",
    "\n",
    "test_sentences = [\n",
    "    \"Selamat pagi! Apa kabar? #Tanya###>Good morning! How are you? #Question\",\n",
    "    \"Saya suka bermain sepak bola, terutama di akhir pekan. @Sport###>I love playing soccer, especially on weekends. @Football\",\n",
    "    \"Kita harus pergi ke pasar sekarang! #Urgent###>We have to go to the market now! #Important\",\n",
    "    \"Buku ini sangat menarik, dan saya tidak bisa berhenti membacanya. %Menarik###>This book is very interesting, and I can't stop reading it. %Interesting\",\n",
    "    \"Apa pendapatmu tentang film itu? *Pikirkan###>What do you think about that movie? *Think about it\",\n",
    "    \"Hati-hati! Jalan licin, terutama saat hujan. #HatiHati###>Be careful! The road is slippery, especially when it rains. #Caution\",\n",
    "    \"Dia berkata, 'Saya akan datang besok.' #Janji###>He said, 'I will come tomorrow.' #Promise\",\n",
    "    \"Saya tidak suka makanan pedas; lebih baik yang manis. #Pilih###>I don't like spicy food; I prefer sweet. #Choice\",\n",
    "    \"Mengapa kamu terlambat? Apakah ada masalah? @Tanya###>Why are you late? Is there a problem? @Question\",\n",
    "    \"Bisa kita bicara nanti? Saya sibuk saat ini. @Sibuk###>Can we talk later? I'm busy right now. @Busy\",\n",
    "    \"Cinta itu aneh, bukan? Terkadang, sulit untuk dipahami. *Cinta###>Love is strange, isn't it? Sometimes, it's hard to understand. *Love\",\n",
    "    \"Dia membeli apel, jeruk, dan pisang. %Belanja###>She bought apples, oranges, and bananas. %Shopping\",\n",
    "    \"Wow! Itu luar biasa, bukan? #Wow###>Wow! That's amazing, isn't it? #Amazing\",\n",
    "    \"Kamu tidak bisa pergi begitu saja; kita harus membahas ini. @Bahas###>You can't just leave; we need to discuss this. @Discuss\",\n",
    "    \"Setiap orang berhak atas pendapatnya, bukan? *Pendapat###>Everyone has the right to their opinion, right? *Opinion\"\n",
    "]\n",
    "\n",
    "for test in test_sentences:\n",
    "    out = tokenize(test)\n",
    "    print(out)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef9ce1-24a5-409f-beb3-b6953fb62d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tqdm(ds_train[\"text\"]):\n",
    "    tks = tokenize(x)\n",
    "    for t in tks[\"id\"]:\n",
    "        vocab[\"id\"].add(t)\n",
    "    for t in tks[\"eng\"]:\n",
    "        if \"#\" in t:\n",
    "            print(t)\n",
    "        vocab[\"eng\"].add(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9e60a9ee-f3b4-46c7-820e-e0dd8a6278f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[\"id\"] = sorted(set(vocab[\"id\"]))\n",
    "vocab[\"eng\"] = sorted(set(vocab[\"eng\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d48cf29-6304-463c-939f-1a5daff0394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[\"eng\"][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cab6d6af-004f-430a-b007-4e4bd2064e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_clean = Dataset.load_from_disk(\"./train_clean.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a8f5b736-6be1-47b8-9390-83868088f675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Mundurlah, bajingan. ###>Back the fuck up!'}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1437b735-4e90-4beb-aa96-93210bd7ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_enc = Dataset.load_from_disk(\"./train_encoded.hf\")[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "fc24cdb4-5e0d-4b34-bdea-0cfd1e2ec231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'eng': [7321, 100075, 73516, 103053, 0], 'id': [44829, 5, 73544, 1892]},\n",
       " {'eng': [41906, 75949, 56394, 74605, 100306, 56538, 79193, 1569],\n",
       "  'id': [54130, 1892, 1892, 1892, 51455, 73486, 1892]},\n",
       " {'eng': [55770, 2, 91096, 105604, 1569], 'id': [32653, 122144, 1892]},\n",
       " {'eng': [9833,\n",
       "   5,\n",
       "   104739,\n",
       "   68663,\n",
       "   105834,\n",
       "   75949,\n",
       "   100823,\n",
       "   93840,\n",
       "   72869,\n",
       "   105904,\n",
       "   3516],\n",
       "  'id': [12396, 5, 72306, 93431, 70724, 117645, 132271, 84233, 3694]},\n",
       " {'eng': [49941, 82806, 1569], 'id': [32561, 1892]}]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_enc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "853c5085-2b4f-410d-a0fc-55475723394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.coder import Coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ad3c51d1-bf6c-49f1-90af-2fdfa29ca26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coder_id = Coder(vocab[\"id\"])\n",
    "coder_eng = Coder(vocab[\"eng\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "33428462-ef26-4fb0-8357-15939f3f30f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coder_eng.decode([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2f4f4f22-b013-490c-ba01-34a7ed1b1390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44829, 5, 73544, 1892] [7321, 100075, 73516, 103053, 0]\n",
      "[54130, 1892, 1892, 1892, 51455, 73486, 1892] [41906, 75949, 56394, 74605, 100306, 56538, 79193, 1569]\n",
      "[32653, 122144, 1892] [55770, 2, 91096, 105604, 1569]\n",
      "[12396, 5, 72306, 93431, 70724, 117645, 132271, 84233, 3694] [9833, 5, 104739, 68663, 105834, 75949, 100823, 93840, 72869, 105904, 3516]\n",
      "[32561, 1892] [49941, 82806, 1569]\n",
      "[34662, 111279, 81411, 78894, 115673, 1892] [54100, 2, 81375, 97587, 105243, 100075, 92820, 66254, 1569]\n",
      "[47044, 5, 71276, 127642, 91428, 1892] [37108, 5, 25228, 80157, 79193, 1569]\n",
      "[64681, 77539, 1892] [25228, 1569, 1569, 1569, 62503, 2, 99256, 1569]\n",
      "[17059, 72306, 133818, 81865, 123862, 81770, 74154, 102266, 103708, 94786, 73452, 91428, 74643, 1892] [5452, 104739, 76000, 96133, 102415, 100075, 99294, 65481, 100075, 89336, 100035, 59849, 79193, 104253, 101748, 1569]\n",
      "[52023, 0] [25099, 0]\n",
      "[68634, 5, 133346, 5, 133346, 1892] [54555, 5, 104938, 5, 104938, 1569]\n",
      "[26980, 91428, 1892] [14320, 79193, 86715, 1569]\n",
      "[27958, 122402, 92002, 1892] [50251, 80638, 79136, 64020, 1569]\n",
      "[32653, 101999, 90525, 3694] [55770, 64634, 103053, 3516]\n",
      "[10628, 1892] [12009, 1569]\n",
      "[17059, 95720, 125280, 122070, 133681, 70724, 81770, 99466, 80699, 81770, 78507, 1892] [5452, 100823, 24327, 60192, 104942, 79136, 77977, 100075, 76150, 57884, 69579, 1569]\n",
      "[17338, 122135, 1892] [20426, 105885, 88888, 1569]\n",
      "[27439, 1892] [34331, 1569]\n",
      "[48916, 71130, 109318, 117247, 123782, 111914, 1892] [35184, 83038, 105086, 58938, 76632, 96875, 61311, 1569]\n",
      "[6031, 51813, 1892] [55770, 2, 91096, -1, 1569]\n"
     ]
    }
   ],
   "source": [
    "for x in ds_enc[:20][\"text\"]:\n",
    "    id, eng = x[\"id\"], x[\"eng\"]\n",
    "\n",
    "    print(id, eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7b5e0ceb-c5b8-4930-8cd9-131cf49a4e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mundurlah', ',', 'bajingan', '.'] ['Back', 'the', 'fuck', 'up', '!']\n",
      "['Rebecca', '.', '.', '.', 'Pikirkanlah', 'baik-baik', '.'] ['Rebecca', 'have', 'a', 'good', 'think', 'about', 'it', '.']\n",
      "['Kau', 'salah', '.'] ['You', \"'\", 're', 'wrong', '.']\n",
      "['Brook', ',', 'apa', 'kau', 'ada', 'perkataan', 'untuk', 'dirimu', '?'] ['Brook', ',', 'what', 'do', 'you', 'have', 'to', 'say', 'for', 'yourself', '?']\n",
      "['Katakan', '.'] ['Tell', 'me', '.']\n",
      "['Kita', 'mulai', 'dengan', 'cambuk', 'pendek', '.'] ['We', \"'\", 'll', 'start', 'with', 'the', 'riding', 'crop', '.']\n",
      "['Oh', ',', 'aku', 'tahu', 'itu', '.'] ['Oh', ',', 'I', 'know', 'it', '.']\n",
      "['Tidak', 'bisa', '.'] ['I', '.', '.', '.', 'can', \"'\", 't', '.']\n",
      "['Dan', 'apa', 'yg', 'dia', 'selipkan', 'di', 'bawah', 'meja', 'membuktikan', 'kekuatan', 'bahwa', 'itu', 'benar', '.'] ['And', 'what', 'he', 'slid', 'under', 'the', 'table', 'convinced', 'the', 'powers', 'that', 'be', 'it', 'was', 'true', '.']\n",
      "['Pondok', '!'] ['Hut', '!']\n",
      "['Whoa', ',', 'whoa', ',', 'whoa', '.'] ['Whoa', ',', 'whoa', ',', 'whoa', '.']\n",
      "['Hentikan', 'itu', '.'] ['Cut', 'it', 'out', '.']\n",
      "['Hukumnya', 'sangat', 'jelas', '.'] ['The', 'law', 'is', 'clear', '.']\n",
      "['Kau', 'mau', 'ikutan', '?'] ['You', 'coming', 'up', '?']\n",
      "['Bersulang', '.'] ['Cheers', '.']\n",
      "['Dan', 'kepunyaan-Nya-lah', 'siapa', 'saja', 'yang', 'ada', 'di', 'langit', 'dan', 'di', 'bumi', '.'] ['And', 'to', 'Him', 'belongs', 'whoever', 'is', 'in', 'the', 'heavens', 'and', 'earth', '.']\n",
      "['Dari', 'sakumu', '.'] ['From', 'your', 'pocket', '.']\n",
      "['Hmm', '.'] ['Mm', '.']\n",
      "['Pasukanku', 'akan', 'menyerang', 'perbatasan', 'selatan', 'negaranya', '.'] ['My', 'men', 'will', 'attack', 'his', 'southern', 'border', '.']\n",
      "['Anda', 'Plettschner', '.'] ['You', \"'\", 're', -1, '.']\n"
     ]
    }
   ],
   "source": [
    "for x in ds_enc[:20][\"text\"]:\n",
    "    id, eng = x[\"id\"], x[\"eng\"]\n",
    "    \n",
    "\n",
    "    print(coder_id.decode(id), coder_eng.decode(eng))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e4b362c7-7a0a-4560-901d-80bff91be4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['!'], ['!'])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coder_id.decode([0]), coder_eng.decode([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "28750dc9-b939-4b00-99c3-25e059899b47",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[191], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ds_enc[:\u001b[38;5;241m20\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mid\u001b[39m, eng \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m], x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mid\u001b[39m \u001b[38;5;241m==\u001b[39m coder_id\u001b[38;5;241m.\u001b[39mencode(coder_id\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mid\u001b[39m)),\n\u001b[0;32m----> 5\u001b[0m           \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m==\u001b[39m ds_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\n\u001b[1;32m      6\u001b[0m          )\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(eng \u001b[38;5;241m==\u001b[39m coder_eng\u001b[38;5;241m.\u001b[39mencode(coder_eng\u001b[38;5;241m.\u001b[39mdecode(eng)))\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(ds_enc[:20][\"text\"]):\n",
    "    id, eng = x[\"id\"], x[\"eng\"]\n",
    "\n",
    "    print(id == coder_id.encode(coder_id.decode(id)),\n",
    "          id == ds_clean[\"text\"][\"id\"][i]\n",
    "         )\n",
    "\n",
    "\n",
    "    print(eng == coder_eng.encode(coder_eng.decode(eng)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c2fec294-7f5f-43ae-9cdf-f14d91ee9fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import ENGtoID\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c1482de1-4ac4-4065-bce4-a07d75b92775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7da4dfca-fa88-40ff-9d77-48e884b88072",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ENGtoID(valid=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "04dfe5fb-b2e7-450d-a80e-3f749fefc145",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'eng'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[214], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset[i]\n",
      "File \u001b[0;32m~/dev/uni/ict303/assignment2/dataloader.py:17\u001b[0m, in \u001b[0;36mENGtoID.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[0;32m---> 17\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     19\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(x)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'eng'"
     ]
    }
   ],
   "source": [
    "dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "324593e0-e2b2-453a-8f7d-2f355680c18e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[218], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n\u001b[1;32m     22\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ENGtoID(valid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mToTensor())\n\u001b[0;32m---> 24\u001b[0m dataset[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[218], line 19\u001b[0m, in \u001b[0;36mENGtoID.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     17\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(x)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "File \u001b[0;32m~/miniconda3/envs/doml/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[0;32m~/miniconda3/envs/doml/lib/python3.12/site-packages/torchvision/transforms/functional.py:142\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    140\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (F_pil\u001b[38;5;241m.\u001b[39m_is_pil_image(pic) \u001b[38;5;129;01mor\u001b[39;00m _is_numpy(pic)):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy(pic) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'list'>"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset as torchDataset\n",
    "\n",
    "from datasets import Dataset as dsDataset\n",
    "\n",
    "class ENGtoID(torchDataset):\n",
    "    def __init__(self, valid=False, transform=None):\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        dataset_path = \"./valid_encoded.hf\" if valid else \"./train_encoded.hf\"\n",
    "        self.data = dsDataset.load_from_disk(dataset_path)[\"text\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.data[i][\"eng\"], self.data[i][\"id\"]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "dataset = ENGtoID(valid=False, transform=ToTensor())\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6a10e4c3-10b6-4e88-9a48-afd28a8cd930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5, 6, 7, 8]), tensor([5., 6., 7., 8.]))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = [5, 6, 7, 8]\n",
    "torch.tensor(z), torch.Tensor(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
